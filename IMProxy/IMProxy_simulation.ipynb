{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26517b4a-ad36-4510-9d3c-7c65f487c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dpkt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from scipy.spatial import distance as dist\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from scipy import stats\n",
    "import matplotlib\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8a54baf-6d37-4cfa-b251-06bb93a744c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TransitionPeriod = 0.5\n",
    "LENGTH = 14400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806eaa97-60d6-4ebe-ae21-9812c4bd5198",
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH = 3600\n",
    "HOUR_secs = 3600\n",
    "download_speed = 30000000\n",
    "upload_speed = 25000000\n",
    "DELTA = 15\n",
    "delay_scales = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 1, 5]\n",
    "delay_scales = [0.5, 1.0, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9f1535-d46c-4090-9b35-0318f424555b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_message_file_names(b, e, messages_dir):\n",
    "    all_messages_file_names = sorted(os.listdir(all_messages_path))\n",
    "    if not os.path.exists(messages_dir):\n",
    "        os.makedirs(messages_dir)\n",
    "    for i in range(len(all_messages_file_names)):\n",
    "        if i >= b and i <= e:\n",
    "            print(all_messages_file_names[i])\n",
    "            os.system('cp {} {}'.format(os.path.join(all_messages_path, all_messages_file_names[i]), os.path.join(messages_dir,all_messages_file_names[i])))\n",
    "    return\n",
    "\n",
    "all_messages_path = 'path/to/generated_messages/messages_pareto_xm_5000_alpha_0.93_max30delay_ver02'\n",
    "root_dir = 'path/to/traces/directory'\n",
    "timestamps_dirs = []\n",
    "messages_dirs = []\n",
    "message_file_names = []\n",
    "packets_pickle_dirs = []\n",
    "packets = []\n",
    "dir_count = 0\n",
    "for path in Path(root_dir).iterdir():\n",
    "    if path.is_dir() and path.name != 'aggregate' and path.name != 'old':\n",
    "        print(path)\n",
    "        timestamps_dirs.append(os.path.join(path, 'timestamps'))\n",
    "        messages_dirs.append(os.path.join(path, 'messages'))\n",
    "        range_begin = int(path.name.split('_')[0])\n",
    "        range_end = int(path.name.split('_')[1])\n",
    "        print(range_begin, range_end)\n",
    "        if len(os.listdir(all_messages_path)) == 0:\n",
    "            get_message_file_names(range_begin, range_end, messages_dirs[dir_count])\n",
    "        message_file_names.extend(sorted(os.listdir(messages_dirs[dir_count])))\n",
    "        packets_pickle_dirs.append(os.path.join(path, 'traces/pickles/packets'))\n",
    "        # read all packets from packets pickle and write them to 'packets'\n",
    "        pickle_name = os.listdir(packets_pickle_dirs[dir_count])[0]\n",
    "        print(pickle_name)\n",
    "        with open(os.path.join(packets_pickle_dirs[dir_count], pickle_name[:-7] + '.pickle'), 'rb') as handle:\n",
    "            packets.extend(pickle.load(handle))\n",
    "            print(packets[-1])\n",
    "        dir_count += 1\n",
    "aggregate_dir = Path(root_dir) / 'aggregate'\n",
    "aggregate_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir = (aggregate_dir / 'countermeasures')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir = str(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888f1b06-7250-4b2a-a621-9536ea5358d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sizes_sender(messages_file, max_count=None):\n",
    "    sizes = []\n",
    "    with open(messages_file) as f:\n",
    "        data = json.load(f)\n",
    "        count = 0\n",
    "        for message in data['messages']:\n",
    "            if max_count is None or count < max_count:\n",
    "                sizes.append(int(message['size'])) # sizes are in bytes\n",
    "            else:\n",
    "                break\n",
    "            count += 1\n",
    "    return sizes\n",
    "\n",
    "def get_timestamps_sender(timestamps_file, max_count=None):\n",
    "    timestamps = []\n",
    "    with open(timestamps_file) as f:\n",
    "        lines = f.readlines()\n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            if max_count is None or count < max_count:\n",
    "                line = line.split()\n",
    "                timestamps.append((float)(line[2]))\n",
    "            else:\n",
    "                break\n",
    "            count += 1\n",
    "    return timestamps\n",
    "\n",
    "def get_sizes_of_all_senders(messages_dir):\n",
    "    message_file_names = sorted(os.listdir(messages_dir))\n",
    "    all_message_sizes_of_senders = []\n",
    "    for mf_name in message_file_names:\n",
    "        message_sizes = get_sizes_sender(os.path.join(messages_dir, mf_name))\n",
    "        all_message_sizes_of_senders.append(message_sizes)\n",
    "    return all_message_sizes_of_senders\n",
    "\n",
    "def get_timestamps_of_all_senders(messages_dir, timestamps_root_dir):\n",
    "    message_file_names = sorted(os.listdir(messages_dir))\n",
    "    all_timestamps = []\n",
    "    for mf_name in message_file_names:\n",
    "        timestamps = get_timestamps_sender(os.path.join(timestamps_root_dir, 'timestamps_' + mf_name[:-5] + '.txt'))\n",
    "        all_timestamps.append(timestamps)\n",
    "    return all_timestamps\n",
    "\n",
    "def get_types_of_all_senders(messages_dir, max_count=None):\n",
    "    message_file_names = sorted(os.listdir(messages_dir))\n",
    "    all_types = []\n",
    "    for messages_file in message_file_names:\n",
    "        with open(os.path.join(messages_dir, messages_file)) as f:\n",
    "            types = []\n",
    "            data = json.load(f)\n",
    "            count = 0\n",
    "            for message in data['messages']:\n",
    "                if max_count is None or count < max_count:\n",
    "                    types.append((message['type']))\n",
    "                else:\n",
    "                    break\n",
    "                count += 1\n",
    "            all_types.append(types)\n",
    "    return all_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915689d2-6f1b-4d01-8bd5-ae29bf05a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_of_all_senders = []\n",
    "message_sizes_of_all_senders = []\n",
    "message_types_of_all_senders = []\n",
    "for i in range(dir_count):\n",
    "    timestamps_of_all_senders.extend(get_timestamps_of_all_senders(messages_dirs[i], timestamps_dirs[i]))\n",
    "    message_sizes_of_all_senders.extend(get_sizes_of_all_senders(messages_dirs[i]))\n",
    "    message_types_of_all_senders.extend(get_types_of_all_senders(messages_dirs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8ea3598-8819-4bac-ac66-68705d46ab7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_receiver_packets(packets, timestamps_of_all_senders):\n",
    "    packets_of_all_receivers = []\n",
    "    all_beginnings = []\n",
    "    all_endings = []\n",
    "    for sender_timestamps in timestamps_of_all_senders:\n",
    "        beginning = sender_timestamps[0] - 5 #seconds\n",
    "        ending = sender_timestamps[len(sender_timestamps)-1] + 30 #seconds\n",
    "        all_beginnings.append(beginning)\n",
    "        all_endings.append(ending)\n",
    "    print(all_beginnings)\n",
    "    print(all_endings)\n",
    "    current_sender = 0\n",
    "    num_of_senders = len(timestamps_of_all_senders)\n",
    "    current_sender_received_packets = []\n",
    "    for p in packets:\n",
    "        if p.get('timestamp') > all_beginnings[current_sender] and p.get('timestamp') < all_endings[current_sender]:\n",
    "            current_sender_received_packets.append(p)\n",
    "        elif p.get('timestamp') >= all_endings[current_sender]:\n",
    "            packets_of_all_receivers.append(current_sender_received_packets)\n",
    "            if current_sender < num_of_senders - 1:\n",
    "                current_sender += 1\n",
    "                current_sender_received_packets = []\n",
    "                if p.get('timestamp') > all_beginnings[current_sender] and p.get('timestamp') < all_endings[current_sender]:\n",
    "                    current_sender_received_packets.append(p)\n",
    "            else:\n",
    "                break\n",
    "    return packets_of_all_receivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36cc398-f0d8-4577-9584-94fef6f69ca8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "packets_of_all_receivers = separate_receiver_packets(packets, timestamps_of_all_senders)\n",
    "packets_towards_receivers = [[p for p in packets if (p.get('to_amazon') is False)] for packets in packets_of_all_receivers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9808073a-64c9-4147-8127-6e13512dc738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time(timestamps, start_time):\n",
    "    converted_timestamps = []\n",
    "    for t in timestamps:\n",
    "        converted = t - start_time\n",
    "        if converted < 0:\n",
    "            print('Error: timestamp is larger than start time with t = {} and start_time = {}'.format(t, start_time))\n",
    "        converted_timestamps.append(converted)\n",
    "    return converted_timestamps    \n",
    "\n",
    "def get_on_periods_of_sender_2(timestamps, message_sizes):\n",
    "    global TransitionPeriod\n",
    "    bursts = []\n",
    "    for t, s in zip(timestamps, message_sizes):\n",
    "        bursts.append((t, s))\n",
    "    return bursts\n",
    "\n",
    "def get_on_periods_of_user(timestamps, user_packet_sizes):\n",
    "    global TransitionPeriod\n",
    "    is_in_burst = False\n",
    "    bursts = []\n",
    "    burst_size = 0\n",
    "    last_burst_index = 0\n",
    "    for time_iter in range(0, len(timestamps)):\n",
    "        if time_iter == len(timestamps) - 1 and is_in_burst == True:\n",
    "            if burst_size > 1514:\n",
    "                bursts.append((timestamps[last_burst_index], burst_size))\n",
    "                break\n",
    "        if timestamps[time_iter] - timestamps[last_burst_index] > TransitionPeriod:\n",
    "            if is_in_burst == True:\n",
    "                if burst_size > 1514:\n",
    "                    bursts.append((timestamps[last_burst_index], burst_size))\n",
    "                is_in_burst = False\n",
    "                burst_size = 0\n",
    "        if user_packet_sizes[time_iter] > 1400:\n",
    "            if is_in_burst == False:\n",
    "                is_in_burst = True\n",
    "                last_burst_index = time_iter\n",
    "            burst_size += user_packet_sizes[time_iter]\n",
    "    return bursts\n",
    "\n",
    "def get_bursts_user(timestamps, user_packet_sizes):\n",
    "    global TransitionPeriod\n",
    "    is_in_burst = False\n",
    "    bursts = []\n",
    "    burst_size = 0\n",
    "    last_burst_index = 0\n",
    "    first_burst_index = 0\n",
    "    for time_iter in range(0, len(timestamps)):\n",
    "        if time_iter == len(timestamps) - 1 and is_in_burst == True:\n",
    "            if burst_size > 2000:\n",
    "                bursts.append((timestamps[first_burst_index], timestamps[last_burst_index], burst_size))\n",
    "                break\n",
    "        if timestamps[time_iter] - timestamps[last_burst_index] > 0.5:\n",
    "            if is_in_burst == True:\n",
    "                if burst_size > 2000:\n",
    "                    bursts.append((timestamps[first_burst_index], timestamps[last_burst_index], burst_size))\n",
    "                is_in_burst = False\n",
    "                burst_size = 0\n",
    "        if user_packet_sizes[time_iter] > 1400:\n",
    "            if is_in_burst == False:\n",
    "                is_in_burst = True\n",
    "                first_burst_index = time_iter\n",
    "            last_burst_index = time_iter\n",
    "            burst_size += user_packet_sizes[time_iter]\n",
    "    return bursts\n",
    "\n",
    "def get_bursts_user_2(timestamps, user_packet_sizes):\n",
    "    bursts = []\n",
    "    burst_size = 0\n",
    "    for time_iter in range(0, len(timestamps) - 1):\n",
    "        ipd = timestamps[time_iter + 1] - timestamps[time_iter]\n",
    "        if ipd < 1:\n",
    "            if user_packet_sizes[time_iter] > 40:\n",
    "                burst_size += user_packet_size[time_iter]\n",
    "        else:\n",
    "            if burst_size > 20000:\n",
    "                bursts.append((timestamps[time_iter], burst_size))\n",
    "            burst_size = 0\n",
    "    return bursts\n",
    "\n",
    "def merge_channel_period_points(ChannelPeriodPoints):\n",
    "    c = []\n",
    "    time = [x[0] for x in ChannelPeriodPoints]\n",
    "    size = [x[1] for x in ChannelPeriodPoints]\n",
    "    burst_time = []\n",
    "    burst_volume = []\n",
    "    burst_volume.append(size[0])\n",
    "    burst_time.append(time[0])\n",
    "    temp = time[0]\n",
    "    for i in range(len(time)-1):\n",
    "        if time[i+1] - temp < 2:\n",
    "            burst_volume[-1] += size[i+1]\n",
    "            temp = time[i+1]\n",
    "        else:\n",
    "            burst_time.append(time[i+1])\n",
    "            burst_volume.append(size[i+1])\n",
    "            temp = time[i+1]\n",
    "    for i in range(len(burst_time)):\n",
    "        c.append((burst_time[i], burst_volume[i]))\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62141f08-e57b-4c8a-b3ff-274abb6b2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intervals(period_points, minute):\n",
    "    global LENGTH\n",
    "    num_of_intervals = int(LENGTH / minute)\n",
    "    intervals = []\n",
    "    for i in range(num_of_intervals):\n",
    "        intervals.append([])\n",
    "        startpoint = i * minute\n",
    "        endpoint = (i + 1) * minute\n",
    "        for j in range(len(period_points)):\n",
    "            if startpoint <= period_points[j][0] and period_points[j][0] < endpoint:\n",
    "                intervals[-1].append(period_points[j])\n",
    "    return intervals\n",
    "\n",
    "def find_intervals_channel(period_points, minute, message_types):\n",
    "    global LENGTH\n",
    "    num_of_intervals = int(LENGTH / minute)\n",
    "    intervals = []\n",
    "    for i in range(num_of_intervals):\n",
    "        intervals.append([])\n",
    "        startpoint = i * minute\n",
    "        endpoint = (i + 1) * minute\n",
    "        for j in range(len(period_points)):\n",
    "            if startpoint <= period_points[j][0] and period_points[j][0] < endpoint and message_types[j] != 'text':\n",
    "                intervals[-1].append(period_points[j])\n",
    "    return intervals\n",
    "\n",
    "def find_matches(channel_interval, user_interval):\n",
    "    number_of_matches = 0\n",
    "    number_of_nonmatches = 0\n",
    "    matched_user_intervals = set()\n",
    "    for j in range(min(len(channel_interval), len(user_interval))):\n",
    "        time = channel_interval[j][0]\n",
    "        size = channel_interval[j][1]\n",
    "        is_matched = False\n",
    "        for d in range(1, DELTA):\n",
    "            for i in range(len(user_interval)):\n",
    "    #             print (time, user_interval[i][0], DELTA)\n",
    "                if user_interval[i] in matched_user_intervals:\n",
    "                    continue\n",
    "                if abs(user_interval[i][0] - time) < d:\n",
    "                    if abs(size - user_interval[i][1]) < 0.25 * size:\n",
    "                        matched_user_intervals.add(user_interval[i])\n",
    "                        number_of_matches += 1\n",
    "                        is_matched = True\n",
    "                        break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        if is_matched is False:\n",
    "            number_of_nonmatches += 1\n",
    "    return number_of_matches / float(number_of_matches + number_of_nonmatches)\n",
    "\n",
    "def detect_correlations_from_bursts(channel_bursts, user_bursts, message_types, j):\n",
    "    global observation_lengths\n",
    "    correlations = []\n",
    "    user_intervals = find_intervals(user_bursts, observation_lengths[j])\n",
    "    channel_intervals = find_intervals_channel(channel_bursts, observation_lengths[j], message_types)\n",
    "    correlation_of_intervals = detect_correlations_from_intervals(channel_intervals, user_intervals)\n",
    "    return correlation_of_intervals\n",
    "\n",
    "def detect_correlations_from_intervals(channel_intervals, user_intervals):\n",
    "    correlation = []\n",
    "    for i in range(len(channel_intervals)):\n",
    "        if len(channel_intervals[i]) == 0 and len(user_intervals[i]) == 0: # no events in either one\n",
    "            correlation.append(-1)\n",
    "        elif len(channel_intervals[i]) == 0 or len(user_intervals[i]) == 0: # only one has an event\n",
    "            correlation.append(0)\n",
    "        else:\n",
    "            correlation.append(find_matches(channel_intervals[i], user_intervals[i]))\n",
    "    return correlation\n",
    "\n",
    "def remove_empty_correlations(correlations):\n",
    "    organized = []\n",
    "    for i in range(len(correlations)):\n",
    "        organized.append([c for c in correlations[i] if c != -1])\n",
    "    return organized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f92d42-0f2d-4801-8621-482bf1962554",
   "metadata": {},
   "source": [
    "## Delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8240818d-fdce-43bd-96d0-aed105d75b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delay(timestamps, scale):\n",
    "    exp_delay = np.random.exponential(scale, size = len(timestamps))\n",
    "    delay_sum = np.sum(exp_delay)\n",
    "    for i in range(len(exp_delay)-1, -1, -1):\n",
    "        timestamps[i] += delay_sum\n",
    "        delay_sum -= exp_delay[i]\n",
    "    return timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62f17924-09cc-43c1-8d91-f44af4d16d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_bursts(bursts, mode):\n",
    "    new_bursts = []\n",
    "    for i in range(len(bursts)):\n",
    "        bursts_temp = []\n",
    "        for b in bursts[i]:\n",
    "            if mode == 'receiver':\n",
    "                bursts_temp.append((b[1], b[2]))\n",
    "            if mode == 'sender':\n",
    "                bursts_temp.append((b[0], b[2]))\n",
    "        new_bursts.append(bursts_temp)\n",
    "    return new_bursts\n",
    "\n",
    "def get_all_bursts_delayed(scale):\n",
    "    all_user_bursts = []\n",
    "    all_channel_bursts = []\n",
    "    print('scale is {}'.format(scale))\n",
    "    for i in range(0, len(packets_towards_receivers)):\n",
    "        print('Hour {} {}'.format(i, message_file_names[i]))\n",
    "\n",
    "        channel_timestamps = timestamps_of_all_senders[i]\n",
    "        channel_message_sizes = message_sizes_of_all_senders[i]\n",
    "\n",
    "\n",
    "        user_packet_timestamps = [p.get('timestamp') for p in packets_towards_receivers[i]]\n",
    "        user_packet_sizes = [p.get('size') for p in packets_towards_receivers[i]]\n",
    "\n",
    "        if len(user_packet_timestamps) == 0:\n",
    "            all_user_bursts.append([])\n",
    "            all_channel_bursts.append([])\n",
    "            print('ERROR')\n",
    "            continue\n",
    "        start_time = min(channel_timestamps[0], user_packet_timestamps[0])\n",
    "\n",
    "        converted_sender_timestamps = convert_time(channel_timestamps, start_time)\n",
    "\n",
    "        sender_upload_times = [size / upload_speed for size in channel_message_sizes]\n",
    "        sender_end_upload_timestamps = [t + d for t,d in zip(converted_sender_timestamps, sender_upload_times)]\n",
    "        sender_period_points = list(tuple(zip(converted_sender_timestamps, sender_end_upload_timestamps, channel_message_sizes)))\n",
    "\n",
    "        converted_user_timestamps = convert_time(user_packet_timestamps, start_time)\n",
    "        delayed_converted_receiver_timestamps = delay(converted_user_timestamps, scale)\n",
    "        user_period_points = get_bursts_user(converted_user_timestamps, user_packet_sizes)\n",
    "\n",
    "        all_user_bursts.append(user_period_points)\n",
    "        all_channel_bursts.append(sender_period_points)\n",
    "        \n",
    "        print ('Hour {} is done'.format(i))\n",
    "        print (\"\\n\")\n",
    "        \n",
    "\n",
    "    return all_channel_bursts, all_user_bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5b6b78c3-f3fe-4bba-9678-f591a610bafa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tp_fp_delayed_for_scale(scale):\n",
    "    all_channel_bursts, all_user_bursts = get_all_bursts_delayed(scale)\n",
    "    with open(Path(results_dir) / 'delayed' / 'pickles' / 'all_user_bursts_delta_{}_delayed_{:.2f}.pickle'.format(DELTA, scale), 'wb') as handle:\n",
    "        pickle.dump(all_user_bursts, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(Path(results_dir) / 'delayed' / 'pickles' / 'all_channel_bursts_delta_{}_delayed_{:.2f}.pickle'.format(DELTA, scale), 'wb') as handle:\n",
    "        pickle.dump(all_channel_bursts, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    all_channel_bursts = get_relevant_bursts(all_channel_bursts, 'sender')\n",
    "    all_user_bursts = get_relevant_bursts(all_user_bursts, 'receiver')\n",
    "    \n",
    "    \n",
    "    all_correlations_tp = []\n",
    "#     for j in range(len(observation_lengths)):\n",
    "    j = 3\n",
    "    print('For {}-second interval:'.format(observation_lengths[j]))\n",
    "    corrs_tp = []\n",
    "    for i in range(len(all_user_bursts)): # for every hour\n",
    "        corrs = detect_correlations_from_bursts(all_channel_bursts[i], all_user_bursts[i],\n",
    "                                                message_types_of_all_senders[i], j)\n",
    "        if corrs != -1:\n",
    "            corrs_tp.extend(corrs)\n",
    "        print (\"Hour {} is done\".format(i))\n",
    "    all_correlations_tp.append([c for c in corrs_tp if c != -1])\n",
    "    with open(Path(results_dir) / 'delayed'/ 'pickles' / 'corrs_tp_event_based_delta_{}_delay_{:.2f}.pickle'.format(DELTA, scale), 'wb') as handle:\n",
    "        pickle.dump(all_correlations_tp, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    \n",
    "    all_corrs_fp = []\n",
    "#     for m in range(len(observation_lengths)):\n",
    "    m = 3\n",
    "    print('For {}-second interval:'.format(observation_lengths[m]))\n",
    "    corrs_fp = []\n",
    "    for i in range(len(all_user_bursts)):\n",
    "        for j in range(len(all_channel_bursts)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            corrs = detect_correlations_from_bursts(all_channel_bursts[j], all_user_bursts[i], message_types_of_all_senders[j], m)\n",
    "            if len(corrs) == 0:\n",
    "                print('warning')\n",
    "            if corrs != -1:\n",
    "                corrs_fp.extend(corrs)\n",
    "            print (\"user {} with channel {} is done\".format(i, j))\n",
    "    all_corrs_fp.append([c for c in corrs_fp if c != -1])\n",
    "    with open(Path(results_dir) / 'delayed'/ 'pickles' /'corrs_fp_event_based_delta_{}_delay_{:.2f}.pickle'.format(DELTA, scale), 'wb') as handle:\n",
    "        pickle.dump(all_corrs_fp, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('scale {} is done'.format(scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76c1416-a560-41f8-8691-de581306cd47",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_tp_fp_delayed_for_scale(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b42fc9-4be1-4455-8064-7d3d5e5f9779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "201580f0-69fe-49ea-accb-0894ce55478d",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dc74132e-e939-43ff-a0f0-831ae405b51f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "photos_dir = 'path/to/generated/media/photos'\n",
    "\n",
    "with open(Path(results_dir) / 'padding'/ 'pickles' / 'photo_sizes.pickle', 'rb') as handle:\n",
    "    photo_sizes = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "52b23093-278b-4711-a72d-147c9ccf169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_bursts(bursts, padding_ratio):\n",
    "    padded_bursts = []\n",
    "    for b in bursts:\n",
    "        random_ratio = np.random.rand()\n",
    "        added_size = b[2] * random_ratio * padding_ratio\n",
    "        added_time = added_size  / download_speed\n",
    "        padded_bursts.append((b[0], b[1] + added_time, b[2] + added_size))\n",
    "    return sorted(padded_bursts, key = lambda tup: tup[1])\n",
    "\n",
    "def generate_dummy_bursts(prob = 0.002):\n",
    "    dummy_bursts = []\n",
    "    timestamp = 0\n",
    "    while timestamp < 3600:\n",
    "        r = np.random.rand()\n",
    "        if r < prob:\n",
    "            random_size = np.random.choice(photo_sizes)\n",
    "            added_time = random_size / download_speed\n",
    "            if timestamp + added_time >= 3600:\n",
    "                break\n",
    "            else:\n",
    "                dummy_bursts.append((timestamp, timestamp + added_time, random_size))\n",
    "                timestamp += added_time\n",
    "        else:\n",
    "            s += 1\n",
    "    return dummy_bursts\n",
    "\n",
    "def add_padding_and_dummy_packets(bursts, padding_ratio, prob):\n",
    "    combined_bursts = pad_bursts(bursts, padding_ratio)\n",
    "    combined_bursts.extend(generate_dummy_bursts(prob))\n",
    "    combined_bursts = sorted(combined_bursts, key = lambda tup: tup[1])\n",
    "    return combined_bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bba327a2-9214-4ebf-beae-707d273ae003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_bursts(bursts, mode):\n",
    "    new_bursts = []\n",
    "    for i in range(len(bursts)):\n",
    "        bursts_temp = []\n",
    "        for b in bursts[i]:\n",
    "            if mode == 'receiver':\n",
    "                bursts_temp.append((b[1], b[2]))\n",
    "            if mode == 'sender':\n",
    "                bursts_temp.append((b[0], b[2]))\n",
    "        new_bursts.append(bursts_temp)\n",
    "    return new_bursts\n",
    "\n",
    "def get_all_bursts_padded(padding_ratio, dummy_prob):\n",
    "    all_user_bursts = []\n",
    "    all_channel_bursts = []\n",
    "    print('Padding ratio is {}'.format(padding_ratio))\n",
    "    for i in range(0, len(packets_towards_receivers)):\n",
    "\n",
    "        print('Hour {} {}'.format(i, message_file_names[i]))\n",
    "\n",
    "        channel_timestamps = timestamps_of_all_senders[i]\n",
    "        channel_message_sizes = message_sizes_of_all_senders[i]\n",
    "\n",
    "\n",
    "        user_packet_timestamps = [p.get('timestamp') for p in packets_towards_receivers[i]]\n",
    "        user_packet_sizes = [p.get('size') for p in packets_towards_receivers[i]]\n",
    "\n",
    "        if len(user_packet_timestamps) == 0:\n",
    "            all_user_bursts.append([])\n",
    "            all_channel_bursts.append([])\n",
    "            print('ERROR')\n",
    "            continue\n",
    "        start_time = min(channel_timestamps[0], user_packet_timestamps[0])\n",
    "\n",
    "        converted_sender_timestamps = convert_time(channel_timestamps, start_time)\n",
    "\n",
    "        sender_upload_times = [size / upload_speed for size in channel_message_sizes]\n",
    "        sender_end_upload_timestamps = [t + d for t,d in zip(converted_sender_timestamps, sender_upload_times)]\n",
    "        sender_period_points = list(tuple(zip(converted_sender_timestamps, sender_end_upload_timestamps, channel_message_sizes)))\n",
    "\n",
    "        converted_user_timestamps = convert_time(user_packet_timestamps, start_time)\n",
    "        user_period_points = get_bursts_user(converted_user_timestamps, user_packet_sizes)\n",
    "        padded_user_period_points = pad_bursts(user_period_points, padding_ratio)\n",
    "\n",
    "\n",
    "        all_user_bursts.append(padded_user_period_points)\n",
    "        all_channel_bursts.append(sender_period_points)\n",
    "        print ('Hour {} is done'.format(i))\n",
    "        print (\"\\n\")\n",
    "        \n",
    "    return all_channel_bursts, all_user_bursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "57ca98fa-6f98-403a-af8b-1d0911ac78fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tp_fp_with_padding(padding_ratio, dummy_prob):\n",
    "    all_channel_bursts, all_user_bursts = get_all_bursts_padded(padding_ratio, dummy_prob)\n",
    "    all_channel_bursts = get_relevant_bursts(all_channel_bursts, 'sender')\n",
    "    all_user_bursts = get_relevant_bursts(all_user_bursts, 'receiver')\n",
    "    \n",
    "    \n",
    "    all_correlations_tp = []\n",
    "    for j in range(len(observation_lengths)):\n",
    "        print('For {}-second interval:'.format(observation_lengths[j]))\n",
    "        corrs_tp = []\n",
    "        for i in range(len(all_user_bursts)): # for every hour\n",
    "            corrs = detect_correlations_from_bursts(all_channel_bursts[i], all_user_bursts[i],\n",
    "                                                    message_types_of_all_senders[i], j)\n",
    "            if corrs != -1:\n",
    "                corrs_tp.extend(corrs)\n",
    "            print (\"Hour {} is done\".format(i))\n",
    "        all_correlations_tp.append([c for c in corrs_tp if c != -1])\n",
    "    with open(Path(results_dir) / 'padding'/ 'pickles' / 'corrs_tp_event_based_delta_{}_padding_ratio_{:.2f}.pickle'.format(DELTA, padding_ratio), 'wb') as handle:\n",
    "        pickle.dump(all_correlations_tp, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    \n",
    "    all_corrs_fp = []\n",
    "    for m in range(len(observation_lengths)):\n",
    "        print('For {}-second interval:'.format(observation_lengths[m]))\n",
    "        corrs_fp = []\n",
    "        for i in range(len(all_user_bursts)):\n",
    "            for j in range(len(all_channel_bursts)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                corrs = detect_correlations_from_bursts(all_channel_bursts[j], all_user_bursts[i], message_types_of_all_senders[j], m)\n",
    "                if len(corrs) == 0:\n",
    "                    print('warning')\n",
    "                if corrs != -1:\n",
    "                    corrs_fp.extend(corrs)\n",
    "                print (\"user {} with channel {} is done\".format(i, j))\n",
    "        all_corrs_fp.append([c for c in corrs_fp if c != -1])\n",
    "    with open(Path(results_dir) / 'padding'/ 'pickles' / 'corrs_fp_event_based_delta_{}_padding_ratio_{:.2f}.pickle'.format(DELTA, padding_ratio), 'wb') as handle:\n",
    "        pickle.dump(all_corrs_fp, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('Padding ratio {} is done'.format(padding_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262fc29-d999-472f-a1a5-e68ad5af9120",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_tp_fp_with_padding(1, 0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0a6521-3448-490f-84fb-7d354aaa3e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:messaging] *",
   "language": "python",
   "name": "conda-env-messaging-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
