{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import tqdm\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version 1.8.0\n",
      "cuda version 11.1\n",
      "1\n",
      "cuda:0\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "Memory Usage:\n",
      "Allocated:    0.0 GB\n",
      "Cached:       0.0 GB\n",
      "Total memory: 10.8 GB\n"
     ]
    }
   ],
   "source": [
    "print('torch version', torch.__version__)\n",
    "print('cuda version',torch.version.cuda)\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\", 0)\n",
    "print (device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name())\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:   ', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:      ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "    print('Total memory:', round(torch.cuda.get_device_properties(0).total_memory/1024**3,1),'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "matplotlib.rc('xtick', labelsize=20) \n",
    "matplotlib.rc('ytick', labelsize=20)\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_idx = 2\n",
    "batch_size = 125\n",
    "num_epoches = 4\n",
    "negative_samples = 4 # how many false for every correct sample\n",
    "scale = 0.5\n",
    "pad_ratio = 0.5\n",
    "pb = 0.0005\n",
    "minutes = [180, 300, 900, 1800]\n",
    "flowSize = minutes[min_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_wire_normal_no_photos = 'path/to/traces/for/training/and/validation'\n",
    "results_dir = Path(root_dir_wire_normal_no_photos) / 'deepcorr'\n",
    "pickles_dir = results_dir / 'pickles'\n",
    "with open(results_dir /  'all_user_flows.pickle', 'rb') as handle:\n",
    "    all_user_flows = pickle.load(handle)\n",
    "with open(results_dir / 'all_channel_flows.pickle', 'rb') as handle:\n",
    "    all_channel_flows = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "userFlows = np.array(all_user_flows[min_idx]) # 2 for 900\n",
    "advFlows = np.array(all_channel_flows[min_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((268, 46, 900), (268, 46, 900))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userFlows.shape, advFlows.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(userFlows)\n",
    "rr= list(range(length))\n",
    "np.random.shuffle(rr)\n",
    "train_index = rr[:200]\n",
    "test_index = rr[200:] # validation\n",
    "\n",
    "# Uncomment when running for the first time:\n",
    "# with open(pickles_dir / 'Wire_train_test_indices_deepcorr.pickle', 'wb') as f:\n",
    "#     pickle.dump((train_index, test_index), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pickles_dir / 'Wire_train_test_indices_deepcorr.pickle', 'rb') as handle:\n",
    "    (train_index, test_index) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(user_flows, adv_flows, train_index):\n",
    "\n",
    "    x_train = np.zeros((len(train_index)*user_flows.shape[1]*(negative_samples+1), 1, 2, flowSize))\n",
    "\n",
    "    y_train = np.zeros((len(train_index)*user_flows.shape[1]*(negative_samples+1), 1))\n",
    "\n",
    "    print ('x_train shape is {}'.format(x_train.shape))\n",
    "\n",
    "    index = 0\n",
    "    random_ordering = [] + train_index\n",
    "    for i in tqdm.tqdm(train_index):\n",
    "        for j in range(len(userFlows[i])):\n",
    "        \n",
    "            x_train[index, 0, 0, :] = user_flows[i, j, :]\n",
    "            x_train[index, 0, 1, :] = adv_flows[i, j, :]\n",
    "\n",
    "            if index % (negative_samples+1) != 0:\n",
    "                print (index)\n",
    "                raise\n",
    "            y_train[index, 0] = 1\n",
    "            m = 0\n",
    "            index += 1\n",
    "            np.random.shuffle(random_ordering)\n",
    "            for idx in random_ordering:\n",
    "                if idx == i or m>(negative_samples-1):\n",
    "                    continue\n",
    "                m += 1\n",
    "                x_train[index, 0, 0, :] = user_flows[idx, j, :]\n",
    "                x_train[index, 0, 1, :] = adv_flows[i, j, :]\n",
    "\n",
    "                y_train[index,0] = 0\n",
    "                index += 1\n",
    "    return x_train, y_train\n",
    "\n",
    "x_test = np.zeros((len(test_index)*len(test_index)*userFlows.shape[1], 1, 2, flowSize))\n",
    "\n",
    "y_test = np.zeros((len(test_index)*len(test_index)*userFlows.shape[1], 1))\n",
    "\n",
    "\n",
    "index = 0\n",
    "for i in tqdm.tqdm(test_index):\n",
    "    for k in range(len(userFlows[i])):\n",
    "        for j in test_index:\n",
    "            x_test[index, 0, 0, :] = userFlows[i, k, :]\n",
    "            x_test[index, 0, 1, :] = advFlows[j, k, :]\n",
    "\n",
    "            if i == j:\n",
    "                y_test[index, 0] = 1\n",
    "            else:\n",
    "                y_test[index, 0] = 0\n",
    "\n",
    "            index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1000, (2,40), stride=2)\n",
    "        self.max_pool1 = nn.MaxPool2d((1,4), stride=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(1000, 400, (1,20), stride=2)\n",
    "        self.conv2_bn = nn.BatchNorm2d(400)\n",
    "        self.max_pool2 = nn.MaxPool2d((1,4), stride=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(80800, 3000)\n",
    "\n",
    "        self.fc2 = nn.Linear(3000, 800)\n",
    "        self.fc3 = nn.Linear(800, 100)\n",
    "        self.fc4 = nn.Linear(100, 1)\n",
    "    \n",
    "    def weight_init(self):\n",
    "        for m in self._modules:\n",
    "            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "                m.weight.data.normal_(0.0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, inp, dropout_prob):\n",
    "        x = inp\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pool1(x)\n",
    "        x = F.relu(self.conv2_bn(self.conv2(x)))\n",
    "        x = self.max_pool2(x)\n",
    "        \n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.dropout2d(x, p=dropout_prob)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.dropout2d(x, p=dropout_prob)\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.dropout2d(x, p=dropout_prob)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainData(Dataset):\n",
    "    def __init__(self, x_train, y_train):\n",
    "        \n",
    "        self.x = x_train\n",
    "        self.y = y_train\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x_train_cuda = torch.from_numpy(self.x[index]).float().to(device)\n",
    "        y_train_cuda = torch.from_numpy(self.y[index]).float().to(device)\n",
    "        return x_train_cuda, y_train_cuda\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "\n",
    "class TestData(Dataset):\n",
    "    def __init__(self, x_test):\n",
    "        \n",
    "        self.x = x_test\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x_test_cuda = torch.from_numpy(self.x[index]).float().to(device)\n",
    "        return x_test_cuda\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestData(x_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    running_loss=0\n",
    "    train_corrs= np.zeros((len(train_index)*userFlows.shape[1]*(negative_samples+1), 1))\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data, 0.4) # dropout = 0.4\n",
    "        loss = F.binary_cross_entropy_with_logits(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        o = torch.sigmoid(outputs)\n",
    "        corrs = o.data.cpu().numpy()\n",
    "        train_corrs[batch_idx*batch_size:(batch_idx+1)*batch_size] = corrs\n",
    "        if batch_idx % 100 == 0:\n",
    "            print (\"Loss: {:0.6f} for batch_idx {}\".format(loss.item(), batch_idx))       \n",
    "    train_loss=running_loss/len(train_loader)\n",
    "    print(\"Training loss: {} for epoch\".format(train_loss))\n",
    "    l = train_loss\n",
    "    return l, train_corrs\n",
    "\n",
    "def test(model, device, test_loader, index):\n",
    "    test_corrs = np.zeros((len(index)*len(index)*userFlows.shape[1], 1))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(test_loader):\n",
    "            if data.size(0) != batch_size: ### ?\n",
    "                print('size is {}'.format(data.size(0)))\n",
    "                continue\n",
    "            output = model(data, 0.0)\n",
    "            o = torch.sigmoid(output)\n",
    "            corrs = o.data.cpu().numpy()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print (\"batch_idx {}\".format(batch_idx))\n",
    "            test_corrs[batch_idx*batch_size:(batch_idx+1)*batch_size] = corrs\n",
    "    return test_corrs\n",
    "            \n",
    "    \n",
    "def cal_tp_fp(corrs):\n",
    "    th = np.arange(0,1,0.1)\n",
    "    fp = []\n",
    "    tp = []\n",
    "    diag = np.diagonal(corrs)\n",
    "    for t in th:\n",
    "        tp_temp = np.sum(diag[diag >= t]) / float(len(diag))\n",
    "        tp.append(tp_temp)\n",
    "        fp_temp =0\n",
    "        for i in range(corrs.shape[0]):\n",
    "            for j in range(corrs.shape[0]):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                else:\n",
    "                    if corrs[i,j] >= t:\n",
    "                        fp_temp += 1\n",
    "        fp.append(float(fp_temp) / (corrs.shape[0]*corrs.shape[1] - len(corrs)))\n",
    "        \n",
    "    return (tp,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "model.weight_init()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_acc = 0\n",
    "last_loss = 100\n",
    "for epoch in range(num_epoches):\n",
    "    \n",
    "    print ('Epoch: ', epoch)\n",
    "    \n",
    "    x_train, y_train = generate_data(userFlows, advFlows, train_index)\n",
    "    \n",
    "    train_dataset = TrainData(x_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    print('now calculating loss')\n",
    "    loss, train_corrs = train(model, device, train_loader, optimizer)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for idx in range(int(len(x_train) / len(train_index))):\n",
    "        best=np.argmax(train_corrs[idx*(len(train_index)):(idx+1)*(len(train_index))])\n",
    "\n",
    "        if y_train[best+(idx*(len(train_index)))]==1:\n",
    "            tp+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "    acc = float(tp) / float(tp+fp)\n",
    "    print ('train', fp, tp, acc)\n",
    "    print('now testing (validation)')  \n",
    "    corrs = test(model, device, test_loader, test_index)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for idx in range(int(len(x_test) / len(test_index))):\n",
    "        best=np.argmax(corrs[idx*(len(test_index)):(idx+1)*(len(test_index))])\n",
    "\n",
    "        if y_test[best+(idx*(len(test_index)))]==1:\n",
    "            tp+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "    acc = float(tp) / float(tp+fp)\n",
    "    print ('test (validation)', fp, tp, acc)\n",
    "    if acc > 0.4 and loss < last_loss:\n",
    "        print (\"saving...\")\n",
    "        torch.save(model.state_dict(), results_dir / 'models/{}/wire-{}-epoch{}-acc{:.2f}-nsp{}.pth.tar'.format(flowSize, flowSize, epoch, acc, negative_samples))\n",
    "        \n",
    "        print (\"saved for epoch {} and loss {} and accuracy {}\".format(epoch, loss, acc))\n",
    "        last_acc = acc\n",
    "        last_loss = loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "messaging",
   "language": "python",
   "name": "messaging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
